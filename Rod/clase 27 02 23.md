# Binomios

$(a+b) =1$
$(a+b)^{1} =a+b$
$(a+b)^{2}= (a+b) (a+b)$
	$\vdots$ 
$(a+b)^{n}=(a+b)^{n-1}(a+b)$ 

sustituyendo apoyandonos del triangulo de pascal es:
%%$(a+b)^{4}\sum\limits$%%
$(a+b)^{4}= a^{4}+4a^{3}b+6a^{2}b^{2}+4ab^{3}+b^{4}$


Definición: Un histograma es una función que asigna a cada elemento de una partición de un intervalo $[a,b]$ el número de miembros de un conjunto de datos que pertenecen a dicho elemento.

D: Conjunto de datos
$[a,b] = U_{i=1}J_{i},~~j_{i}\cap J_{j}=0$ 
$hisT:J_{i}\rightarrow n_{i}$

Proposicion.
Sean los datos D los valores una variable aleatoria X con función de densidad $f_{X}(x)$.
Entonces $F_{X}(x)=n_{i}$ donde n; es el número de datos que eprtenecen a un intervalo $J_{i}$ deuna partición, que contiene a X.

Prueba.
Sea $(J_{i})^{2}$ una partición uniforme del intervalo $[0,1]$ con $\mid J_{i}\mid=\frac{1}{N}$, (donde n es el número de muestras generadas), entonces la probabilidad  de que X caiga en $J_{i}$ es $P\left(a_{i}\leq X \leq b_{i}=O_{i}+\frac{1}{N} \right) =F(b_{i})-F(a_{i})$. Por otro lado $P(a_{i}\leq X\leq b_{i})=\frac{n_{i}}{N}$ 

$$F(b_{i})-F(a_{i}) = \frac {n_{i}} {N},~~~~\frac{[F(b_{i})-F(a_{i})]} {\frac{i}{N}=b_{i}-a_{i}}=n_{i}$$ 
$lim_{N\rightarrow\infty} \frac{F(b_{i})-F(a_{i})}{b_{i}-a_{i}}=F_{X}(a_{i})$ 

###### Varianza
$$var(X) = \mathbb{E} [X-\mathbb{E}(X)^{2}]=\mathbb{E}[x^{2}-2X\mathbb{E}[X]+\mathbb{E}[X]^{2}]$$
$$=\mathbb{E}[X^{2}]-2\mathbb{E}[X]\mathbb{E}[X]+\mathbb{E}[X]^{2}=\mathbb{E}[X^{2}]-\mathbb{E^2}[X]$$
$$Var(X)=\mathbb{E}[X^{2}]-\mathbb{E^{2}}[X]$$
<hr>
$VAR(cX)=E[(cX-E[cX])^{2}]=\mathbb{E}[(cX-c\mathbb{E}[x])^{2}]= \mathbb{E} [(c(X-\mathbb{E}[X])^{2}=\mathbb{E}[c^{2}(X-\mathbb{E}[X])^{2}]=c^{2}\mathbb{E}[(X-\mathbb{E} [X])^{2}]$
$Var(cX)=c^{2}Var(X)$

###### Definición
Si x,y son variables aletorias continuas con función de destribucion conjunta $f_{xy}(x,y)$ decimos que son independientes si obtiene que $f_{xy} (x,y)=f_{x}(x)f_{y}(y)$ 

Proposición si X,Y son variables aleatorias continuas independientes entonces>
$E[XY]=\mathbb{E}[X]\mathbb{E}[Y]$

Prueba.
$$\mathbb{E}[x,y]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(xy)f_{XY}(x,y)d_{x}d_{y}=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x,y)f_{X}(x)f_{Y}(y)d_{x}d_{y}$$
$$=\mathbb{E}[Y]\mathbb{E}[X]$$

$Cov(X,Y) =\mathbb{E}[(x-\mathbb{E}[x]) (y-\mathbb{E}[y])]=\mathbb{E}[xy-x\mathbb{E}[Y]-\mathbb{E}[X]Y+\mathbb{E}[X]\mathbb{E}[Y]]=\mathbb{E}[xy]-\mathbb{E}[y]\mathbb{E}[x]-\mathbb{E}[Y]\mathbb{E}[x]+\mathbb{E}[y]=\mathbb{E}[XY]+\mathbb{E}[x]\mathbb{E}[Y]-2\mathbb{E}[X]\mathbb{E}[Y]$
$\mathbb{E}[X]\mathbb{E}[Y]$

###### Proposicion
Si dos variables aleatorias continuas X,Y son independientes, entonces su $cov(x,y)=0$

$$var(X+Y)=var(x) ~var(y)$$

Prueba.
$$var(x+y) = \mathbb{E}[((x+y) - \mathbb{E}[x+y])^{2}] = E[X]+E[Y]$$
$$=E{(x-E[x])}^{2}+(y+E[y])^{2}+2(x-E[x])(y-E[y])$$
$$=E[(x-E[x])^{2}] +E[(y+E[y])^{2}]+2E[(x-E[x]) (y-E(x))] $$
$$=var(x+y)=var(x)+var(y)$$

Definicion.
Decimos que una variable aleatoria descreta es de Bernoulli si vale 1 cuando ocurre un evento conn probabilidad p y q con probabilidad (1-p) cunado no ocurre el evento.

Sea y una variable aleatoria discreta de Bernoulli entonces:
- $E[y] = P,~~var(y)=P(p-1)$ 
Prueba.
$\mathbb{E}[Y] = 1 P +0(1-P)= P$
$Var(Y) = \mathbb{E}[(\frac{y}{\mathbb{E}[Y])^{2}]=}P(1-p)^{2}+(1-p)(0.p)^{2}=p(1-p)^{2}+(1-p)p^{2}=(1-p)[p(1-p)+p^{2}]=(1-p)[p-p^{2}+p^{2}]=p(1-p)$
Si X es una variable aleatoria binomial con probabilidad $\begin{pmatrix}n \\ k\end{pmatrix} p^{n}(1-p)^{n-k}$ para n ensayos. Entonces X se puede representar como la suma de n variables aleatorias de Bernoulli de pendientes $X=Y+y+\cdots+Y\rightarrow n~veces$
$\mathbb{E}[X]=\mathbb{E}[Y]+\cdots+\mathbb{E}[Y]=n\mathbb{E}[Y]=np$
$var[X]=var[y]+\cdots+var[Y]=np(p-1)$ 