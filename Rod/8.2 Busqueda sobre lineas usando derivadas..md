### Método de Bisección
Supongamos que deseamos minimizar una función $\theta$ sobre un intervalo cerrado y acotado. Además, supongamos que $\theta$ es pseudoconvexa.

###### Nota definición:
Considere una función $f: X\leq\mathbb{R}^{n}\rightarrow\mathbb{R}$, definida en un ocnjunto abierto convexo (no vacio) X del espacio euclidiano de dimensión finita $\mathbb{R}^{n}$. Se dice que esta función es pseudoconvexa si> $\forall~x,~y\in X:\Delta~f(x)(y-x)\geq 0 = f(y)\geq~f(x)$ 

Supongamos que estamos en el k-ésimo, es decir este intervalo se ha definido, y además conocemos a esta derivada $\theta' (\lambda_{k})$ entonces consideramos los siguientes tres casos.
1. Si $\theta(\lambda_{k}) = 0$, entonces este punto $(\lambda_{k})$ es un punto mínimo
2. Si $\theta' (\lambda_{k})$, entonces, para $\lambda > \lambda_{k}$ tenemos $\theta' (\lambda_{k})(\lambda-\lambda_{k})>0$ y por la pseudoconvexidad de $\theta$ se sigue que $\theta(\lambda)\geq\theta(\lambda_{k})$, en otras palabras el minimo ocurre a la izquierda de $\lambda_{k}$, es decir el nuevo intervalo de incertidumbre es $a_{k},\lambda_{k}$
3. Si $\theta(\lambda_{k})<0$, entonces, para $\lambda<\lambda_{k}$ entonces * $\theta'(\lambda_{k})~(\lambda -\lambda_{k})>0$. Es decir el mínimo ocurre a la derecha (por $\theta(\lambda) \geq\theta(\lambda_{k})$ por * )  y el nuevo intervalo de incertidumbre es $[\lambda_{k},~b_{k}]$ 

Ahora, consideramos el hecho de que la longitud de los intervalos de incertidumbre debe ser minimizada, luego $\lambda_{k}$ es el punto medio de $[a_{k},~b_{k}]$ 

#### Método de Bisección 
Supongamos que deseamos minimizar una función $\theta$ sobre un intervalo cerrado y acotado, con $\theta$ pseudoconvexa y por tanto, diferensiable. 

Sea $[a_{i},~ b_{i}]$  el intervalo de incertidumbre inicial y sea $\ell$ la longitud minima estblecida. Sea n el entero postivio mas pequeño tal que $(1/2)^{n}<~\ell~/(b_i /a_i)$. Sea k=1 vaya el paso principal.

##### Paso principal
1. Sea $\lambda_{k} = 1/2~(a_{k}+b_{k})$ y evalamos $\theta'(\lambda_{k})$ si $\theta' (\lambda_{k})=0$, pare; $\lambda_{k}$ es una solución óptima. De otra forma maya al paso 2 si $\theta'(\lambda_{k})>0$ y vaya al paso 3 si $\theta'(\lambda_{k})<0$ 
2. Defina $a_{k+1}=a_{k}$ y $b_{k+1}= \lambda_{k}$. Vaya al paso 4
3. Defina $a_{k+1}=\lambda_{k}$ y $b_{k+1}=b_{k}$. Vaya al paso 4
4. Si $k=n$ paramos; el minimo cae en el intervalo $[a_{n+1},~b_{n+1}]$. De otra forma, reemplace k por $k+1$ y repita el paso 1.

$$\frac{b_{i}-a_{i}}{2^{k-1}}= b_{k}-a_{k}$$
$$b_{n}- a_{n}= \frac{b_{i}-a_{i}}{2^{n}}\leq\ell$$
### Método de Newton
Consideramos ahora una función de $C^{2}([a_{i},b_{i}])$ entonces se puede definir $\{ \lambda_{k}\}^{too}_{k=1}$ mediante la ecuación recursiva:
$$\lambda_{k+1}=\lambda_{k}\frac{\theta'(\lambda_{k})}{\theta"(\lambda_{k})}$$

Siempre que $\theta"(\lambda_{k})\neq0$ 

## 5.5 Busqueda multidimensionales sin utilizar derivadas
Abordaremos el problema de minimizar una función f de varias varaibles sin utilizar derivadas. Los métodos en general se presentan como sigue. Dado un vector x, luego se determina un vector d y entonces f es minimizada desde x en la dirección d, utilizando una de las técnicas previamente descritas.
