En esta parte del curso dicutiremos varios procedimientos que están basados en el concepto denominados como conjugación. La noción de conjugación definida abajo es my util en problemas sin restricción.

###### Definición
Sean H una matriz de dimensión nxm, Los vectores $d_{1},\dots,d_{n}$ Son llamados H conjugados o simplente conjugado si son linealmente independientes si $d_{i}^{t}H d_{j}= 0$ para $i\neq j$ como ejemplo particular observemos que sucede en el caso de minimizar funciones cuadráticas.

Consideremos la función cuadrática 
$$f(x)=c^{t}{^x+(1/2)x^{t}~HX}$$
donde H es una matriz simétrica de dimensioón $n~x~n$ y supngamos $(CX_{i},X_{i}~X_{i}^{2}~o~X_{i}X_{j})$ que $d_{i},\dots,d_{n}$ son vectores H conjugados. Por la indepencia lineal de los dís, dado un vector j inicial $X_{i}$, cada punto $X\in\mathbb{R}^n$ puede ser escrito como:
$$X=X_{i}+\sum\limits_{j=1}^{n}\lambda_{j}d_{j}~~~~~*$$
Usando $*$ podemos escribir
$$f(x)=C^{t}CX_{i}+\sum\limits_{j=1}^{n}\lambda_jd_{j}$$
$$f(1/2)=(X_{i}+\frac{n}{z}\lambda_{j}d_{j})H(X_{i}+\frac{n}{z}\lambda_{j}d_{j})=\frac{n}{z}\left[c^{t}(X_{i}+\lambda_{j}d_{j})+\frac{1}{2}(x_{i}+\lambda_jd_{j})^{t}H(X_{i}\lambda_{j}d_{j})\right]$$
La expresion anterior es la más pequeña que podemos obtener, en la suma ambos terminos son de $\lambda$ uno y el segundo termino de $\lambda$ 2 se puede escribir como: $\sum\limits_{j=1}^{n}k_{j}(\lambda_{j})$ 
$$=f(\lambda)$$
Sea $f(x)=c^{t}x+(1/2)x^{t}Hx$, donde H es una matriz simétrica. Sean $d_{1},\dots, d_{n}$ vectores $H_{t}$ Conjugados, y sea $x_{1}$ un punto inicial arbitrario para $k=1,\dots, n$ sea $\lambda_k$ una solución óptima al problema de minimización.
$$f(x_{k}+\lambda d_{k}),~~~~\lambda\in\mathbb{R}$$
Sea $x_{k+1}=x_x+\lambda_{k}d_{k}$. Entonces debemos tener:
1. $\bigtriangledown~f(x_{k+1})d_{j}=0$ 
2. $\bigtriangledown f(x_{1})^{t}d_{k}= \bigtriangledown f(x_{k})^td_{k}$
3. $X_{k+1}$ es una solución óptima al problema de minimización $f(x)$ sujeto a $x-x_{i}\in L(d_{i},\dots,d_{k})$ donde $L(d_{1},\dots,d_{n})$ es el subespacio lineal formado por $d_{1},\dots,d_{k}$. En particular, $X_{n+1}$ es un punto óptima (minimo) sobre $\mathbb{R}^{n}$  


### Método de Davidon-Fletcher-Powell  (DFP)
El siguiente método, es utilizado para minimizar funciones diferenciables de varias variables.

##### Paso inicial
Sea $\epsilon>0$ una tolerancia aceptada escoja $x_{1}$ un punto inicial y una matriz simétrica definida positiva $D_{1}$. SEa $y_{1}= x_{1}$ y $k=j=1$ proceda al paso principal

###### Paso principal
1. Si $\mid\mid\Delta f(y_{j})\mid\mid<\epsilon$ pare si no $d_{j}= -D_{j}\Delta f(y_{j})$ ysea $\lambda_{j}$ una solución ópita una solución óptimaal problema de minimización
$$f(y_{j}+\lambda d_{j})$$
sujeto a $a\geq0$ Defina $y_{j+1}=y_{j}+\lambda_{j}d_{j}$ Si $j<n$ vaya al paso 2. Si $j=n$, redefina $y_{1}=x_{k+1}=y_{n+1}$ 

2. Construya $D_{j+1}$ como sigue:
$$D_{j+1}=D_{j}+\frac{P_{j}D_{j}^{t}}{P_{j}^{t}q_{j}}-\frac{D_{j}q_{j}q_{j}^{t}D_{1}}{q_{j}^{t}D_{j}q_{j}}$$
$P_{j}= \lambda_{j}d_{j}=y_{j+1}-y_{j}$ 
$q_{j}= \bigtriangledown f(y_{j+1})-\bigtriangledown f(y_{j})$

Reemplaze j por $j+1$ y vara al paso 1

pagina 410 del libro

#### Ejercicio
Considere el problema :
minimizar $(x_{1}-2)^{4}+(x_{1}-2x_{2})^{2}$
Calcule $x_{2}$ y $x_{3}$ con $x_{1}=(0,3)$ 

### Método de gradiente conjugado
Los métodos de gradiente conjugado fueron propuesto por Hertener y Stiefel en 1952 para resolver sistemas de ecuaciones.

La idea básica de los métodos de gradiente conjugado para minimizar una función diferenciable $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ es general una sucesión $y_{j}$ definida cmo:
$$j_{j+1}=y_{j}+\lambda_{j}d_{j}$$
donde $d_j$ es la dirección de busqueda y $\lambda_j$ el tamaño de paso que minimiza f sobre $d_j$ desde el punto $y_{j}$ para $j=1$, la dirección de busqueda es $d_{1}= -\bigtriangledown f(y_{1})$ y para iteraciones subsecuentes, dado $y_{j+1}$ con $\bigtriangledown f(y_{1+i})\neq0$ usamos: $d_{j+1}=-\bigtriangledown f(y_{j+1}) + \alpha_{j}~d_{j}$ 
$X_{k+1}= X_{k}+\alpha_{k}~d_{k}$ $-\bigtriangledown f(x_{k})$ 

donde es un parametro que carateriza nuestro nuevo método, obtiene que siempre que $\alpha_{j}\geq0$ podemos escribir:
$$d_{j+1}=\frac{1}{\mu}\left[\mu - [-\bigtriangledown f(y_{j+1})]+(1-\mu)d_{j} \right]$$

donde: $\mu = 1/(1+\alpha_{j})$, se puede demostrar que:
$$\alpha_{j}=\frac{\mid\mid\bigtriangledown f(y_{j+1}) \mid\mid^{2}}{\mid\mid\bigtriangledown f(y_{j})\mid\mid^{2}}$$


### Método del gradiente conjugado Fletcher Beeves.
##### Paso inicial: 
Escoja una tolerancia $\epsilon>0$ y un punto inicial $x_{1}$ sea $y_{1}=x_{1},~d_{1}=-\bigtriangledown f(y_{j}),~k=j=1$ 

##### Paso principal
1. Si $\mid\mid\bigtriangledown f(y_{j})\mid\mid<\epsilon$ pare. De otra forma sea $\lambda_{j}$ la solución óptima al problema $f(y_{j}+\lambda d_{j}),~~\lambda_{j} \geq0$ y sea $y_{j+1}=y_{j}+\lambda_{j}d_{j}$ si $j<n$, vaya al paso 2 de otra forma vaya al paso 3.
$x_{1}= y_{1}$ $x_{2}=b_{1}$ 
$y_{2}= y_{1}+\lambda_{1}d_{1}$ $y_{2}= y_{1}+\lambda_{1}d_{1}$
$y_{3}= y_{2}+\lambda_{2}d_{2}$
$\vdots$
$y_{n}= x_2$ 

2. Sea $d_{j+1}= -\Delta f(y_{j+1})+\alpha_{j}d_{j}$  donde:
$$\alpha_{j}= \frac{\mid\mid\bigtriangledown f(y_{j+1})\mid\mid^{2}}{\mid\mid\bigtriangledown f(y_j)\mid\mid^{2}}$$
reemplaze j por $j+1$ y vaya al paso 1

3. Sea $y_{1}=x_{k+1}=y_{n}$ y sea $d_{1}= -\Delta f(y_{1})$. Sea $j=1$ reemplaze k por $k+1$ y vaya al paso 1.


pagina 424